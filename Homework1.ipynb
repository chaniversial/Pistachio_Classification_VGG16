{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48cf1ddc-fe2e-4f47-87bf-b51d414311ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random, os\n",
    "from glob import glob\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0758372c-3c05-4387-ac25-7b0bda80ce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4531ff9-6ba5-4541-862b-5630167d246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 224 # 저는 VGG16을 참고하여 구현을 하였습니다. 그래서 가로, 세로 224사이즈로 조정하기로 했습니다.\n",
    "HEIGHT = 224\n",
    "CHANNEL = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7090fb5d-4c71-45a2-b53f-ee967d3be827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89efd6f9-b67d-448e-9685-ff1f6307af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6665d818-d80f-4f11-b384-0e6fab61c9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0027.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0022.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0010.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0006.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0018.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/Kirmizi_Pistachio',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/Deep Dream.ipynb',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/keras_model',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0016.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0003.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0036.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0034.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0004.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0037.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0035.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0038.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0028.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0002.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0015.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/GAN2.gif',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/Siirt_Pistachio',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/GAN.gif',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0008.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/Neural Style Transfer.ipynb',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0005.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/pistachio_test.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0020.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0024.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0012.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0014.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0032.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/Homework1.ipynb',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/training_checkpoints',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/Pistachio_Image_Dataset_Request.txt',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/Text Generation.ipynb',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0029.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0021.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0031.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0026.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0025.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/pistachio_test3.jpg',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0019.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0001.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0007.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0030.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0033.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0011.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/GAN.ipynb',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0009.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0023.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0013.png',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/pistachio_test2.jpg',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/samples',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/image_at_epoch_0017.png']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(current_path + \"/*\") # 현재 경로를 생각하여 각각의 이미지에 대한 경로를 확인하기 위해 사용했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a00d6f16-3309-4100-b8a9-915a3081fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kirmizi_Pistachi_Path = '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/Kirmizi_Pistachio/*'\n",
    "Siirt_Pistachio_Path = '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/Siirt_Pistachio/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73229fa9-13b3-4ce8-99a7-7cc671b75b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1232"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kirimi_dataset = glob(Kirmizi_Pistachi_Path) # glob이라는 라이브러리를 활용하여 각각의 파일의 절대 경로를 리스트에 담았습니다.\n",
    "len(kirimi_dataset) # kirimi 데이터셋의 개수를 1232개 였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c91b7c0-938a-46c5-8b27-599f61ccdc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "916"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siirt_dataset = glob(Siirt_Pistachio_Path)\n",
    "len(siirt_dataset) # siirt 데이터셋의 개수는 916개 였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af3aece-089d-4fab-9f89-927dd3f4c1d4",
   "metadata": {},
   "source": [
    "### 두 종류의 데이터셋이 굉장히 적다고 생각하였습니다. \n",
    "### 제가 인공지능에 대해 자세히 알지는 못하지만, 하나의 객체를 확인하기 위해서는 N만장의 데이터가 필요한 것으로 알고있었기 때문입니다.\n",
    "### dogs-vs-cats classification에 관한 공부를 할 때도, 몇만장이 있었던 것으로 기억합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad09dbd-d96e-4cb6-b3a0-1b454252d218",
   "metadata": {},
   "source": [
    "##### 사진 파일 이외에 다른 파일이 있는 찾아보는 과정입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cffdbffc-0b19-4259-8024-d2e279c3b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in kirimi_dataset:\n",
    "    if(i[-3:] != 'jpg'):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c81d3f0-4d56-4c21-b347-2f1a08030e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in siirt_dataset:\n",
    "    if(i[-3:] != 'jpg'):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8ddd43-6807-42bd-b32d-f7ad68460f80",
   "metadata": {},
   "source": [
    "#### 다른 파일이 있으면 무엇이라도 출력이되어야 하는데, 아무것도 출력되지 않습니다.\n",
    "#### 그래서 바로 데이터 전처리에 시행했습니다.\n",
    "#### 앞서 말했듯이 600x600의 이미지를 244x244로 다운 스케일링하고, 채널은 유지했습니다.\n",
    "#### 이미지를 보다 작은 사ㅣ즈로 보간하는 것이기에 Image.LANCZOS 보간법을 사용하여 최대한 원본 이미지를 유지하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aedcff6f-8151-460a-95d2-ba7f25754f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ichanho/.local/share/virtualenvs/Chanho-Ro_Xhmwc/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  \n",
      "/home/ichanho/.local/share/virtualenvs/Chanho-Ro_Xhmwc/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2148 2148\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "answers = []\n",
    "for i in kirimi_dataset:\n",
    "    kirimi_input = Image.open(i)\n",
    "    kirimi_input.convert('RGB')\n",
    "    kirimi_input = kirimi_input.resize((HEIGHT, WIDTH), Image.LANCZOS)\n",
    "    # 이미지를 다운 스케일링할 때 왜곡이 발생하게 되는데, 작은 이미지에 대한 보간법으로 Image.LANCZOS가 가장 적합다는 Pillow Document를 참조하여 사용했습니다.\n",
    "    datasets.append(np.array(kirimi_input))\n",
    "    answers.append(np.array([0, 1])) # 원 핫 인코딩을 하는 과정입니다.\n",
    "\n",
    "for i in siirt_dataset:\n",
    "    siirt_input = Image.open(i)\n",
    "    siirt_input.convert('RGB')\n",
    "    siirt_input = siirt_input.resize((HEIGHT, WIDTH), Image.LANCZOS)\n",
    "    datasets.append(np.array(siirt_input))\n",
    "    answers.append(np.array([1, 0]))\n",
    "    \n",
    "    \n",
    "print(len(datasets), len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e3528-11c4-4ce7-8da3-2fb3743ffc3f",
   "metadata": {},
   "source": [
    "#### Tensorflow 2.6 의 Sequential을 이용하여 모델을 만들고\n",
    "#### model fit을 하기 위해 numpy 데이터로 형변환 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "143ba03b-f11f-4ec4-86f5-41ba8a989a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(datasets)\n",
    "valid = np.array(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8580387f-0dad-46fc-85c2-0b58f3104eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2148, 224, 224, 3)\n",
      "(2148, 2)\n"
     ]
    }
   ],
   "source": [
    "print(test.shape)\n",
    "print(valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0847e550-7ad1-4ebf-9945-a66e47d4499b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f47f3c5-2e48-4530-9c32-bd60b700beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ad5e1-875f-4db2-af27-cce5506890ff",
   "metadata": {},
   "source": [
    "### VGG16 을 기반하여 모델을 구현하였습니다.\n",
    "### 그림을 참조하여 스스로 구현하였습니다.₩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05e1f6ae-d56c-44e6-9452-52768d755433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 22:30:14.035108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-25 22:30:14.054114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-25 22:30:14.054229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-25 22:30:14.054560: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-25 22:30:14.054836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-25 22:30:14.054926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-25 22:30:14.055001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-25 22:30:14.319309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-25 22:30:14.319430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-25 22:30:14.319510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-25 22:30:14.319581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10048 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e346b00e-d76e-434a-847c-473883fd07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64, (3,3), activation=\"relu\", padding='same',input_shape=(HEIGHT, WIDTH , CHANNEL)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(Conv2D(128, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(256, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(Conv2D(512, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(Conv2D(256, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(512, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(Conv2D(512, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(Conv2D(512, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(512, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(Conv2D(512, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(Conv2D(512, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "# model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2,activation=\"softmax\")) # sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565459b4-1194-4035-9ba5-e69a74b75283",
   "metadata": {},
   "source": [
    "### 이번 분류는  A or B 처럼 두 종류의 데이터 타입을 분류하는 것이기에 binary_crossentrophy를 사용하였습니다.ㅏ\n",
    "### optimizer는 RMSprop을 사용하였으며, 학습율은 0.0001로 하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc507302-1039-4a9f-89f9-62fbfd0fcb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 112, 112, 64)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 56, 56, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 56, 56, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 56, 56, 256)       1179904   \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 28, 28, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 512)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 7, 7, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               12845568  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,704,258\n",
      "Trainable params: 28,704,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4), metrics=['accuracy']) # binary_...\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e8e82-935d-4d3f-9542-a3da08ebf741",
   "metadata": {},
   "source": [
    "### 여기는 교수님께서 요청하셨던 데이터의 비율을 7:2:1로 나누는 작업니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdbbc1fc-0512-4244-8c7c-d7d5848d54cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(test, valid, test_size=0.3, shuffle=True, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95153067-02e9-462c-b1e6-a5eeacbdbfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.3, shuffle=True, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "815b8104-6813-439d-a78d-1537288d791f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 22:30:16.012138: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8303\n",
      "2022-05-25 22:30:18.350319: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 23s 370ms/step - loss: 0.8064 - accuracy: 0.7112 - val_loss: 0.4247 - val_accuracy: 0.8093\n",
      "Epoch 2/50\n",
      "47/47 [==============================] - 13s 267ms/step - loss: 0.4632 - accuracy: 0.7984 - val_loss: 0.4584 - val_accuracy: 0.7605\n",
      "Epoch 3/50\n",
      "47/47 [==============================] - 13s 267ms/step - loss: 0.4623 - accuracy: 0.8017 - val_loss: 0.4505 - val_accuracy: 0.8004\n",
      "Epoch 4/50\n",
      "47/47 [==============================] - 13s 273ms/step - loss: 0.4113 - accuracy: 0.8217 - val_loss: 0.3674 - val_accuracy: 0.8226\n",
      "Epoch 5/50\n",
      "47/47 [==============================] - 13s 267ms/step - loss: 0.3940 - accuracy: 0.8283 - val_loss: 0.4103 - val_accuracy: 0.8115\n",
      "Epoch 6/50\n",
      "47/47 [==============================] - 13s 267ms/step - loss: 0.3586 - accuracy: 0.8417 - val_loss: 0.3989 - val_accuracy: 0.8293\n",
      "Epoch 7/50\n",
      "47/47 [==============================] - 13s 268ms/step - loss: 0.3396 - accuracy: 0.8589 - val_loss: 0.3773 - val_accuracy: 0.8182\n",
      "Epoch 8/50\n",
      "47/47 [==============================] - 13s 273ms/step - loss: 0.2972 - accuracy: 0.8736 - val_loss: 0.3576 - val_accuracy: 0.8337\n",
      "Epoch 9/50\n",
      "47/47 [==============================] - 13s 273ms/step - loss: 0.2787 - accuracy: 0.8869 - val_loss: 0.2714 - val_accuracy: 0.8958\n",
      "Epoch 10/50\n",
      "47/47 [==============================] - 13s 272ms/step - loss: 0.2458 - accuracy: 0.9035 - val_loss: 0.2493 - val_accuracy: 0.9113\n",
      "Epoch 11/50\n",
      "47/47 [==============================] - 13s 272ms/step - loss: 0.2075 - accuracy: 0.9148 - val_loss: 0.2477 - val_accuracy: 0.9024\n",
      "Epoch 12/50\n",
      "47/47 [==============================] - 13s 267ms/step - loss: 0.1956 - accuracy: 0.9222 - val_loss: 0.2964 - val_accuracy: 0.8758\n",
      "Epoch 13/50\n",
      "47/47 [==============================] - 13s 268ms/step - loss: 0.1719 - accuracy: 0.9375 - val_loss: 0.8550 - val_accuracy: 0.7539\n",
      "Epoch 14/50\n",
      "47/47 [==============================] - 13s 273ms/step - loss: 0.1627 - accuracy: 0.9388 - val_loss: 0.2111 - val_accuracy: 0.9202\n",
      "Epoch 15/50\n",
      "47/47 [==============================] - 13s 268ms/step - loss: 0.1289 - accuracy: 0.9514 - val_loss: 0.2538 - val_accuracy: 0.9047\n",
      "Epoch 16/50\n",
      "47/47 [==============================] - 13s 268ms/step - loss: 0.1433 - accuracy: 0.9541 - val_loss: 0.2524 - val_accuracy: 0.9047\n",
      "Epoch 17/50\n",
      "47/47 [==============================] - 13s 268ms/step - loss: 0.0914 - accuracy: 0.9701 - val_loss: 0.2384 - val_accuracy: 0.9002\n",
      "Epoch 18/50\n",
      "47/47 [==============================] - 13s 268ms/step - loss: 0.0932 - accuracy: 0.9634 - val_loss: 0.4338 - val_accuracy: 0.8891\n",
      "Epoch 19/50\n",
      "47/47 [==============================] - 13s 268ms/step - loss: 0.0816 - accuracy: 0.9741 - val_loss: 0.3832 - val_accuracy: 0.9069\n",
      "Epoch 19: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f046ee57690>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_path = './keras_model/model.cp'\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5, verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint(cp_path, save_best_only=True, save_weights_only=True)\n",
    "]\n",
    "model.fit(x_train, y_train, epochs=50, validation_data=(x_val, y_val), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cbf73bc-95ea-46cf-8430-ea43f52e7e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 145ms/step - loss: 0.4097 - accuracy: 0.8814\n"
     ]
    }
   ],
   "source": [
    "evaluate_model = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cbf4645-fe36-4e39-9045-8336f8d8ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4624660a-af24-4bf5-b422-6a90cb701d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1503, 224, 224, 3)\n",
      "(1503, 2)\n",
      "(451, 224, 224, 3)\n",
      "(451, 2)\n",
      "(194, 224, 224, 3)\n",
      "(194, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac603977-ca6c-4ce8-b663-35328551cf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_test, verbose=0)\n",
    "print(np.max(pred) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d98b50f-8b5c-42d0-b781-d6b73cf1d91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pred = []\n",
    "for i in pred:\n",
    "    if(i[0] > i[1]):\n",
    "        preprocess_pred.append(1)\n",
    "    else:\n",
    "        preprocess_pred.append(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "140f7d7e-11e5-4a8c-9bab-c986b67d571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_y_test = []\n",
    "for i in y_test:\n",
    "    if(i[0] > i[1]):\n",
    "        preprocess_y_test.append(1)\n",
    "    else:\n",
    "        preprocess_y_test.append(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f7a50ed-55dd-42d8-9a7e-7aacb2b51c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8625\n",
      "Recall:  0.8518518518518519\n",
      "F1-score :   0.8571428571428572\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(preprocess_y_test, preprocess_pred)\n",
    "recall = recall_score(preprocess_y_test, preprocess_pred)\n",
    "f1_scores = f1_score(preprocess_y_test, preprocess_pred)\n",
    " \n",
    "print('Precision: ',precision)\n",
    "print('Recall: ',recall)\n",
    "print('F1-score :  ', f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c8151-608b-4d10-a9d7-6000216cc4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

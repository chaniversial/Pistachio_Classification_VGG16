{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "052f53c7-5a9d-4ad5-a566-7a3e4ccdb27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48cf1ddc-fe2e-4f47-87bf-b51d414311ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random, os\n",
    "from glob import glob\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4531ff9-6ba5-4541-862b-5630167d246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 224 # 저는 VGG16을 참고하여 구현을 하였습니다. 그래서 가로, 세로 224사이즈로 조정하기로 했습니다.\n",
    "HEIGHT = 224\n",
    "CHANNEL = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7090fb5d-4c71-45a2-b53f-ee967d3be827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89efd6f9-b67d-448e-9685-ff1f6307af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6665d818-d80f-4f11-b384-0e6fab61c9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/Kirmizi_Pistachio',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/keras_model',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/Siirt_Pistachio',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/Homework1.ipynb',\n",
       " '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/Pistachio_Image_Dataset_Request.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(current_path + \"/*\") # 현재 경로를 생각하여 각각의 이미지에 대한 경로를 확인하기 위해 사용했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a00d6f16-3309-4100-b8a9-915a3081fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kirmizi_Pistachi_Path = '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/Kirmizi_Pistachio/*'\n",
    "Siirt_Pistachio_Path = '/home/ichanho/workspace/Chanho/Pistachio_Image_Dataset/Pistachio_Image_Dataset/Siirt_Pistachio/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73229fa9-13b3-4ce8-99a7-7cc671b75b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1232"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kirimi_dataset = glob(Kirmizi_Pistachi_Path) # glob이라는 라이브러리를 활용하여 각각의 파일의 절대 경로를 리스트에 담았습니다.\n",
    "len(kirimi_dataset) # kirimi 데이터셋의 개수를 1232개 였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c91b7c0-938a-46c5-8b27-599f61ccdc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "916"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siirt_dataset = glob(Siirt_Pistachio_Path)\n",
    "len(siirt_dataset) # siirt 데이터셋의 개수는 916개 였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af3aece-089d-4fab-9f89-927dd3f4c1d4",
   "metadata": {},
   "source": [
    "### 두 종류의 데이터셋이 굉장히 적다고 생각하였습니다. \n",
    "### 제가 인공지능에 대해 자세히 알지는 못하지만, 하나의 객체를 확인하기 위해서는 N만장의 데이터가 필요한 것으로 알고있었기 때문입니다.\n",
    "### dogs-vs-cats classification에 관한 공부를 할 때도, 몇만장이 있었던 것으로 기억합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad09dbd-d96e-4cb6-b3a0-1b454252d218",
   "metadata": {},
   "source": [
    "##### 사진 파일 이외에 다른 파일이 있는 찾아보는 과정입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cffdbffc-0b19-4259-8024-d2e279c3b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in kirimi_dataset:\n",
    "    if(i[-3:] != 'jpg'):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c81d3f0-4d56-4c21-b347-2f1a08030e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in siirt_dataset:\n",
    "    if(i[-3:] != 'jpg'):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8ddd43-6807-42bd-b32d-f7ad68460f80",
   "metadata": {},
   "source": [
    "#### 다른 파일이 있으면 무엇이라도 출력이되어야 하는데, 아무것도 출력되지 않습니다.\n",
    "#### 그래서 바로 데이터 전처리에 시행했습니다.\n",
    "#### 앞서 말했듯이 600x600의 이미지를 244x244로 다운 스케일링하고, 채널은 유지했습니다.\n",
    "#### 이미지를 보다 작은 사ㅣ즈로 보간하는 것이기에 Image.LANCZOS 보간법을 사용하여 최대한 원본 이미지를 유지하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aedcff6f-8151-460a-95d2-ba7f25754f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ichanho/.local/share/virtualenvs/Chanho-Ro_Xhmwc/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  \n",
      "/home/ichanho/.local/share/virtualenvs/Chanho-Ro_Xhmwc/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2148 2148\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "answers = []\n",
    "for i in kirimi_dataset:\n",
    "    kirimi_input = Image.open(i)\n",
    "    kirimi_input.convert('RGB')\n",
    "    kirimi_input = kirimi_input.resize((HEIGHT, WIDTH), Image.LANCZOS)\n",
    "    \n",
    "    datasets.append(np.array(kirimi_input))\n",
    "    answers.append(np.array([0, 1]))\n",
    "\n",
    "for i in siirt_dataset:\n",
    "    siirt_input = Image.open(i)\n",
    "    siirt_input.convert('RGB')\n",
    "    siirt_input = siirt_input.resize((HEIGHT, WIDTH), Image.LANCZOS)\n",
    "    datasets.append(np.array(siirt_input))\n",
    "    answers.append(np.array([1, 0]))\n",
    "    \n",
    "    \n",
    "print(len(datasets), len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e3528-11c4-4ce7-8da3-2fb3743ffc3f",
   "metadata": {},
   "source": [
    "#### Tensorflow 2.6 의 Sequential을 이용하여 모델을 만들고\n",
    "#### model fit을 하기 위해 numpy 데이터로 형변환 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "143ba03b-f11f-4ec4-86f5-41ba8a989a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(datasets)\n",
    "valid = np.array(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8580387f-0dad-46fc-85c2-0b58f3104eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2148, 224, 224, 3)\n",
      "(2148, 2)\n"
     ]
    }
   ],
   "source": [
    "print(test.shape)\n",
    "print(valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0847e550-7ad1-4ebf-9945-a66e47d4499b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f47f3c5-2e48-4530-9c32-bd60b700beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ad5e1-875f-4db2-af27-cce5506890ff",
   "metadata": {},
   "source": [
    "### VGG16 을 기반하여 모델을 구현하였습니다.\n",
    "### 그림을 참조하여 스스로 구현하였습니다.₩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05e1f6ae-d56c-44e6-9452-52768d755433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 18:14:06.823048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-21 18:14:06.842221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-21 18:14:06.842330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-21 18:14:06.842592: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-21 18:14:06.842868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-21 18:14:06.843047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-21 18:14:06.843120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-21 18:14:07.105619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-21 18:14:07.105744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-21 18:14:07.105827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-21 18:14:07.105899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9869 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e346b00e-d76e-434a-847c-473883fd07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64, (3,3), activation=\"relu\", padding='same',input_shape=(HEIGHT, WIDTH , CHANNEL)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(Conv2D(128, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(256, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(Conv2D(512, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(Conv2D(256, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(512, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(Conv2D(512, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(Conv2D(512, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(512, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(Conv2D(512, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(Conv2D(512, (3,3), activation=\"relu\", padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "# model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2,activation=\"softmax\")) # sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565459b4-1194-4035-9ba5-e69a74b75283",
   "metadata": {},
   "source": [
    "### 이번 분류는  A or B 처럼 두 종류의 데이터 타입을 분류하는 것이기에 binary_crossentrophy를 사용하였습니다.ㅏ\n",
    "### optimizer는 RMSprop을 사용하였으며, 학습율은 0.0001로 하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc507302-1039-4a9f-89f9-62fbfd0fcb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 112, 112, 64)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 56, 56, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 56, 56, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 56, 56, 256)       1179904   \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 28, 28, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 512)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 7, 7, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               12845568  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,704,258\n",
      "Trainable params: 28,704,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4), metrics=['accuracy']) # binary_...\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e8e82-935d-4d3f-9542-a3da08ebf741",
   "metadata": {},
   "source": [
    "### 여기는 교수님께서 요청하셨던 데이터의 비율을 7:2:1로 나누는 작업니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdbbc1fc-0512-4244-8c7c-d7d5848d54cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(test, valid, test_size=0.3, shuffle=True, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95153067-02e9-462c-b1e6-a5eeacbdbfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.3, shuffle=True, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "815b8104-6813-439d-a78d-1537288d791f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 18:14:16.546155: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8303\n",
      "2022-05-21 18:14:18.883462: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 23s 369ms/step - loss: 1.0254 - accuracy: 0.7139 - val_loss: 0.4619 - val_accuracy: 0.8071\n",
      "Epoch 2/50\n",
      "47/47 [==============================] - 12s 266ms/step - loss: 0.4702 - accuracy: 0.7984 - val_loss: 0.5245 - val_accuracy: 0.7539\n",
      "Epoch 3/50\n",
      "47/47 [==============================] - 13s 272ms/step - loss: 0.4426 - accuracy: 0.8017 - val_loss: 0.4163 - val_accuracy: 0.8027\n",
      "Epoch 4/50\n",
      "47/47 [==============================] - 13s 272ms/step - loss: 0.4113 - accuracy: 0.8244 - val_loss: 0.3337 - val_accuracy: 0.8514\n",
      "Epoch 5/50\n",
      "47/47 [==============================] - 13s 268ms/step - loss: 0.3863 - accuracy: 0.8230 - val_loss: 0.3438 - val_accuracy: 0.8359\n",
      "Epoch 6/50\n",
      "47/47 [==============================] - 13s 267ms/step - loss: 0.3560 - accuracy: 0.8450 - val_loss: 0.3391 - val_accuracy: 0.8603\n",
      "Epoch 7/50\n",
      "47/47 [==============================] - 13s 273ms/step - loss: 0.3003 - accuracy: 0.8629 - val_loss: 0.3286 - val_accuracy: 0.8692\n",
      "Epoch 8/50\n",
      "47/47 [==============================] - 13s 268ms/step - loss: 0.2840 - accuracy: 0.8829 - val_loss: 0.3719 - val_accuracy: 0.8204\n",
      "Epoch 9/50\n",
      "47/47 [==============================] - 13s 268ms/step - loss: 0.2739 - accuracy: 0.8842 - val_loss: 0.4981 - val_accuracy: 0.8315\n",
      "Epoch 10/50\n",
      "47/47 [==============================] - 13s 273ms/step - loss: 0.2237 - accuracy: 0.9128 - val_loss: 0.2239 - val_accuracy: 0.9024\n",
      "Epoch 11/50\n",
      "47/47 [==============================] - 13s 273ms/step - loss: 0.1987 - accuracy: 0.9155 - val_loss: 0.2182 - val_accuracy: 0.9157\n",
      "Epoch 12/50\n",
      "47/47 [==============================] - 13s 273ms/step - loss: 0.1697 - accuracy: 0.9395 - val_loss: 0.1904 - val_accuracy: 0.9246\n",
      "Epoch 13/50\n",
      "47/47 [==============================] - 13s 268ms/step - loss: 0.1614 - accuracy: 0.9401 - val_loss: 0.2100 - val_accuracy: 0.9290\n",
      "Epoch 14/50\n",
      "47/47 [==============================] - 13s 268ms/step - loss: 0.1460 - accuracy: 0.9454 - val_loss: 0.2829 - val_accuracy: 0.8980\n",
      "Epoch 15/50\n",
      "47/47 [==============================] - 13s 274ms/step - loss: 0.1439 - accuracy: 0.9594 - val_loss: 0.1878 - val_accuracy: 0.9135\n",
      "Epoch 16/50\n",
      "47/47 [==============================] - 13s 268ms/step - loss: 0.0792 - accuracy: 0.9681 - val_loss: 0.2409 - val_accuracy: 0.9268\n",
      "Epoch 17/50\n",
      "47/47 [==============================] - 13s 268ms/step - loss: 0.1211 - accuracy: 0.9634 - val_loss: 0.3404 - val_accuracy: 0.9180\n",
      "Epoch 18/50\n",
      "47/47 [==============================] - 13s 268ms/step - loss: 0.0877 - accuracy: 0.9714 - val_loss: 0.8094 - val_accuracy: 0.8248\n",
      "Epoch 19/50\n",
      "47/47 [==============================] - 13s 268ms/step - loss: 0.0851 - accuracy: 0.9707 - val_loss: 0.2174 - val_accuracy: 0.9268\n",
      "Epoch 20/50\n",
      "47/47 [==============================] - 13s 268ms/step - loss: 0.0660 - accuracy: 0.9754 - val_loss: 0.2323 - val_accuracy: 0.9313\n",
      "Epoch 20: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1c77723510>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_path = './keras_model/model.cp'\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5, verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint(cp_path, save_best_only=True, save_weights_only=True)\n",
    "]\n",
    "model.fit(x_train, y_train, epochs=50, validation_data=(x_val, y_val), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cbf73bc-95ea-46cf-8430-ea43f52e7e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 143ms/step - loss: 0.4454 - accuracy: 0.9124\n"
     ]
    }
   ],
   "source": [
    "evaluate_model = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf4645-fe36-4e39-9045-8336f8d8ed93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
